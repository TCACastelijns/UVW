{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and load to orignal log of UWV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sad_log_1Q=pd.read_csv('data/sad_log_1Q.csv', encoding= \"ISO-8859-1\")\n",
    "happy_log=pd.read_csv('data/happy_log.csv', encoding= \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sad_log_1Q['asked_question']=1\n",
    "happy_log['asked_question']=0\n",
    "log=pd.concat([happy_log, sad_log_1Q], axis=0)\n",
    "log['sessionid'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create session summary\n",
    "## Aggregate log by sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure the list of activities per session will be ordered by time\n",
    "log.sort_values(['sessionid', 'startTime'], ascending=[True, True], inplace=True)\n",
    "log['startTime']=log['completeTime']\n",
    "\n",
    "# Define aggregations when looking at each session\n",
    "aggregations = {'event': lambda x: list(x), \n",
    "                'case': 'first',\n",
    "                'startTime': 'first', \n",
    "                'completeTime': 'last', \n",
    "                'gender': 'first',\n",
    "                'agecategory': 'first',\n",
    "                'asked_question': 'first',\n",
    "               }\n",
    "sessions = log.groupby('sessionid', as_index=False).agg(aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sessions_customers(sessions, x):\n",
    "    SS=sessions[sessions['case']==x['case']].sort_values('startTime').reset_index(drop=True)\n",
    "    total_sessions=len(SS)\n",
    "    sessions_so_far=SS[SS['sessionid']==x['sessionid']].index[0]\n",
    "    return total_sessions, sessions_so_far\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(tqdm())\n",
    "sessions2=sessions.copy()\n",
    "\n",
    "sessions['total_sessions'], sessions['sessions_so_far'] = zip(*sessions.progress_apply(lambda x: sessions_customers(sessions2, x), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove reoccurences of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eliminate_leakage(event_list, including=True):\n",
    "    \"\"\"\n",
    "    Cut sessions at the point where a question is asked, (including the question itself or not).\n",
    "    If the trace does not include a question, return it unchanged.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        question_index = event_list.index('Question')\n",
    "        if including:\n",
    "            question_index = question_index + 1\n",
    "        else:\n",
    "            question_index = question_index\n",
    "        return event_list[:question_index]\n",
    "    except ValueError:\n",
    "        return event_list\n",
    "    \n",
    "#sessions['event'] = sessions['event'].apply(eliminate_leakage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Age category, Max loops, starthour and gender as additional features to sessions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# If there is a question within a session, return a 1, else a 0\n",
    "def asked_question(event_list):\n",
    "    return int(event_list[-1] == 'Question')\n",
    "\n",
    "# Ordinal mapping of age categories\n",
    "def age(age_cat):\n",
    "    mapping = {'30-39': 35, '50-65': 57, '40-49': 45, '18-29': 23}\n",
    "    return mapping[age_cat]\n",
    "\n",
    "# This function returns:\n",
    "# 1) The most visisted page within the session\n",
    "# 2) The times that page is visited\n",
    "def max_loops(event_list):\n",
    "    num_events=len(event_list)\n",
    "    event_counter = Counter(event_list)\n",
    "    most_visited_page = max(event_counter, key=event_counter.get)\n",
    "    times_visited = event_counter[most_visited_page]\n",
    "    if times_visited == 1:\n",
    "        most_visited_page = None\n",
    "        \n",
    "    inds=[index for index, value in enumerate(event_list) if value==most_visited_page]\n",
    "    if len(inds)==0:\n",
    "        avg_steps=0\n",
    "    else:\n",
    "        steps=np.diff(inds)\n",
    "        avg_steps=np.mean(steps)+1\n",
    "    return most_visited_page, times_visited, avg_steps, num_events\n",
    "\n",
    "# The hour of the timestamp can be seen as a feature\n",
    "def hour(timestamp):\n",
    "    return timestamp.hour\n",
    "\n",
    "# Gender of a customer\n",
    "def gender(gender):\n",
    "    \"\"\"\n",
    "    This could be done directly on the DF but lets keep the same style for everything\n",
    "    \"\"\"\n",
    "    return int(gender == 'M')\n",
    "\n",
    "    \n",
    "# Create target variable - Did this session end up with a question?\n",
    "#sessions['asked_question'] = sessions['event'].apply(asked_question)\n",
    "#sessions['event'] = sessions['event'].apply(eliminate_leakage,including=False)\n",
    "\n",
    "# Gender from character to int\n",
    "sessions['gender'] = sessions['gender'].apply(gender)\n",
    "\n",
    "# Age from category to int and rename column\n",
    "sessions['agecategory'] = sessions['agecategory'].apply(age)\n",
    "sessions.rename(columns={'agecategory': 'age'}, inplace=True)\n",
    "\n",
    "# Hour of day when the session took place.\n",
    "sessions['startTime']=sessions['startTime'].apply(pd.to_datetime)\n",
    "sessions['completeTime']=sessions['completeTime'].apply(pd.to_datetime)\n",
    "sessions['hour'] = sessions['startTime'].apply(hour)\n",
    "\n",
    "timediff=sessions['completeTime']-sessions['startTime']\n",
    "sessions['timediff'] = timediff.apply(lambda x: x.seconds)\n",
    "\n",
    "# Max number of page reoccurence within the sessions and the page mostly visited. \n",
    "# If each page was visited once then mostly visited will be None. The start syntax is interesting,\n",
    "# it allows the apply function to create multiple outputs. This could be useful for the TODO step\n",
    "# mentioned below.\n",
    "sessions['most_visited_page'], sessions['max_loops'], sessions['avg_steps'], sessions['num_events'] = zip(*sessions['event'].apply(max_loops))\n",
    "\n",
    "############################################# TODO ########################################################\n",
    "### We could use the 'most_visited' column to create smart dummy variables. For example something like: ### \n",
    "### Is the home page the mostly visited, or the same for other interesting pages.                       ###\n",
    "############################################# TODO ########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence of trigger events by Markov Chain model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigger_events= ['Visit page mijn_werkmap','Visit page home','Visit page taken',\n",
    "                 'Visit page vacatures_bij_mijn_cv','Visit page mijn_berichten',\n",
    "                 'Visit page werkmap','Visit page mijn_documenten',\n",
    "                 'Visit page mijn_sollicitaties','Visit page mijn_cv',\n",
    "                 'Visit page mijn_tips','Visit page inschrijven',\n",
    "                 'Visit page foutopgetreden.html']\n",
    "\n",
    "# Creating dummy culumns for every trigger event. \n",
    "for event in trigger_events:\n",
    "    sessions[event]=sessions['event'].apply(lambda x: int(event in x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning to predict the presence of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# TARGET VARIABLE\n",
    "y = sessions['asked_question']\n",
    "\n",
    "# Dependent VARIABLES: All possible variables\n",
    "#X = sessions.drop(['asked_question', 'sessionid', 'startTime', 'completeTime', 'most_visited_page', 'event', 'case', 'total_sessions'], axis=1)\n",
    "\n",
    "# Dependent VARIABLES: Just the trigger events\n",
    "X = sessions[trigger_events]\n",
    "\n",
    "features=X.columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just run one of the sessions below (not two of them). In other words you can choose to run:\n",
    "- Random forest\n",
    "- Decision tree\n",
    "- XGBoost (requires manual package installation)\n",
    "\n",
    "Not all of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [2, 3, 4, 5, 6, 7, 8], 'class_weight':[{1: w} for w in [1, 2, 3, 4, 5, 6, 10]]}\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "grid = GridSearchCV(clf, param_grid, cv=3,scoring='roc_auc',n_jobs=3, verbose=3)\n",
    "model=grid.fit(X_train, y_train)\n",
    "    \n",
    "best_parameters, score, _ = max(model.grid_scores_, key=lambda x: x[1])\n",
    "print('Normalized AUC:', score)\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_depth=best_parameters['max_depth'], class_weight= best_parameters['class_weight'], random_state=42)\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state=42)\n",
    "param_grid = {'max_depth': [2, 3, 4], 'class_weight':[{1: w} for w in range(1,20)]}\n",
    "grid = GridSearchCV(clf, param_grid, cv=3,scoring='roc_auc',n_jobs=3, verbose=3)\n",
    "model=grid.fit(X_train, y_train)\n",
    "    \n",
    "best_parameters, score, _ = max(model.grid_scores_, key=lambda x: x[1])\n",
    "print('Normalized AUC:', score)\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth=best_parameters['max_depth'], class_weight= best_parameters['class_weight'], random_state=42)\n",
    "model = model.fit(X_train, np.array(y_train)) \n",
    "\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(model, out_file=None, feature_names=features, class_names=['nQ', 'Q'],filled=True, rounded=True,special_characters=True, impurity=False, rotate=True)  \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"Page Visits_trigger\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth=3, class_weight= best_parameters['class_weight'], random_state=42)\n",
    "model = model.fit(X_train, np.array(y_train)) \n",
    "\n",
    "import graphviz\n",
    "import collections\n",
    "import pydotplus\n",
    "dot_data = tree.export_graphviz(model, out_file=None, feature_names=features, class_names=['nQ', 'Q'],filled=True, rounded=True,special_characters=True, impurity=False, rotate=True)  \n",
    "graph=pydotplus.graph_from_dot_data(dot_data)\n",
    "colors=('blue', 'red')\n",
    "edges = collections.defaultdict(list)\n",
    "\n",
    "for edge in graph.get_edge_list():\n",
    "    edges[edge.get_source()].append(int(edge.get_destination()))\n",
    "\n",
    "for edge in edges:\n",
    "    edges[edge].sort()\n",
    "    src = graph.get_node(edge)[0]\n",
    "    src.get_attributes()['fillcolor']='#e581397f'\n",
    "    total_weight = int(src.get_attributes()['label'].split('samples = ')[1].split('<br/>')[0])\n",
    "    for i in range(2):\n",
    "        dest = graph.get_node(str(edges[edge][i]))[0]\n",
    "        weight = int(dest.get_attributes()['label'].split('samples = ')[1].split('<br/>')[0])\n",
    "        graph.get_edge(edge, str(edges[edge][0]))[0].set_weight((1 - weight / total_weight) * 100)\n",
    "        graph.get_edge(edge, str(edges[edge][0]))[0].set_len(weight / total_weight)\n",
    "        graph.get_edge(edge, str(edges[edge][0]))[0].set_minlen(weight / total_weight)\n",
    "\n",
    "\n",
    "\n",
    "graph = graphviz.Source(dot_data)  \n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb # Requires manual package installation\n",
    "\n",
    "tuning_params = {'max_depth': [3,4,5,6,7,8]}\n",
    "\n",
    "grid = GridSearchCV(xgb.XGBClassifier(objective= 'binary:logistic'), tuning_params, cv=3,scoring='roc_auc',n_jobs=3,verbose=3)\n",
    "model = grid.fit(X_train, y_train)\n",
    "\n",
    "best_parameters, score, _ = max(model.grid_scores_, key=lambda x: x[1])\n",
    "print('Normalized AUC:', score)\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    \n",
    "model = xgb.XGBClassifier(max_depth=best_parameters[param_name])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Threshold for AUC optimization of the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimal_threshold(fpr, tpr, thresholds):\n",
    "    \"\"\"\n",
    "    This funtion select the threshold where the sum of sensitivity and specificity are maximum.\n",
    "\n",
    "    This creiteria is based on:\n",
    "    Bayesian regression methodology for estimating a receiver operating characteristic curve with two radiologic applications: \n",
    "    prostate biopsy and spiral CT of ureteral stones.\n",
    "    \n",
    "    #Parameters:\n",
    "    - From the ROC \n",
    "    a) fpr: false positive rate\n",
    "    b) tpr: true positive rate\n",
    "    c) thresholds: thresholds\n",
    "    \n",
    "    \"\"\"\n",
    "    sensitivity = tpr\n",
    "    specificity = 1 - fpr\n",
    "    opt_threshold = np.argmax(sensitivity + specificity)\n",
    "    opt_fpr = fpr[opt_threshold]\n",
    "    opt_trp = tpr[opt_threshold]\n",
    "    \n",
    "    \n",
    "    return thresholds[opt_threshold], opt_fpr, opt_trp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn\n",
    "#Taken from: http://abhay.harpale.net/blog/machine-learning/threshold-tuning-using-roc/\n",
    "\n",
    "\n",
    "\n",
    "#Predit the prob\n",
    "predictions_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predictions_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "opt_threshold, opt_fpr, opt_trp = optimal_threshold(fpr, tpr, thresholds)\n",
    "\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "plt.figure(figsize=(24, 20))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--',label = 'Random Model')\n",
    "#plt.plot(fpr, thresholds, markeredgecolor='r',linestyle='dashed', color='r', label = 'Threshold')\n",
    "plt.plot(opt_fpr, opt_trp, 'o',linewidth=3)\n",
    "plt.axvline(x = opt_fpr, ymax = opt_trp, linestyle='dashed', color='r'  )\n",
    "plt.axhline(y = opt_trp, xmin =opt_fpr , linestyle='dashed', color='r' ,label = 'Optimal Threshold = %0.2f' % (opt_threshold) )\n",
    "\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#plt.savefig('roc_and_threshold.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_threshold, opt_fpr, opt_trp = optimal_threshold(fpr, tpr, thresholds)\n",
    "predictions= predictions_prob.copy()\n",
    "predictions[predictions<opt_threshold] = 0\n",
    "predictions[predictions>=opt_threshold] = 1\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most important important features according to current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    sns.set(font_scale=2)\n",
    "    plt.figure(figsize=(24, 20))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Important features')\n",
    "    plt.xlabel('Feature importance [0-1]')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "f_importances(model.feature_importances_, features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
