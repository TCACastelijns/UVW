{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "def date_parser(s):\n",
    "    \"\"\"\n",
    "    Parse a date string using the log file's format. For example: '2015/10/16 11:25:59.000'\n",
    "    \"\"\"\n",
    "    without_ms = s.split('.')[0]\n",
    "    return datetime.datetime.strptime(without_ms, '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "log = pd.read_csv('data/log.csv', parse_dates=['startTime', 'completeTime'], date_parser=date_parser)\n",
    "log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Data Quality </h2>\n",
    "\n",
    "Lets start by checking the NaN counts. Are there columns with many missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = log.isnull().sum() * 100 / len(log)\n",
    "null.head()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# the size of A4 paper\n",
    "fig.set_size_inches(5.7, 8.27)\n",
    "\n",
    "sns.barplot(x=null, y=null.index).set_title('NaN % for each column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.heatmap(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Missing value conditional probability </h3>\n",
    "\n",
    "The heatmap above shows features that tend to either existing or missing together. Nullity correlation ranges from -1 (if one variable appears the other definitely does not) to 0 (variables appearing or not appearing have no effect on one another) to 1 (if one variable appears the other definitely also does)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Unique values </h2>\n",
    "\n",
    "Lets now look at the distribution of unique values in each column. This can provide an insight into their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.apply(pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Some of these numbers are interesting. Specifically the following conclusions can be drawn: </b>\n",
    "\n",
    "- The case column refers to the signed in users. There are **4226 unique users** in our database\n",
    "\n",
    "\n",
    "- The sessionid refers to the internet session. There are **106221 different sessions**, therefore each user visited our site **25 times on average** \n",
    "\n",
    "\n",
    "- There are **246 different events**  (all of them referring to specific URL visits). This info is recorded twice (**activity and event columns seem to be the same**). We can therefore safely remove one of them.\n",
    "\n",
    "\n",
    "- Ages are binned into **4 categories** , probably for anonimization purposes.\n",
    "\n",
    "<h3> Lets look with some more detail into the different values for interesting features </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Events without an event type are pretty useless so we might as well drop them\n",
    "log.dropna(axis=0, subset=['event'], inplace=True)\n",
    "\n",
    "events = log['event'].unique()\n",
    "not_page_visits = [event for event in events if not event.startswith('Visit page')]\n",
    "not_page_visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Almost all events are page visits! </h3>\n",
    "\n",
    "As we can see, out of 246 unique events, 243 are visits of specific URL pages. The 3 remaining are very important for our case since we are mostly interested in traces that lead to a call, a question or a complaint. \n",
    "\n",
    "<b> Lets take a closer look at this events. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_percentage(event):\n",
    "    return len(log.loc[log['event'] == event]) * 100 / len(log)\n",
    "\n",
    "for event in not_page_visits:\n",
    "    print('{:.2f}% of records refer to a {}'.format(event_percentage(event), event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complain_cols = [c for c in log.columns if 'complaint' in c] \n",
    "question_cols = [c for c in log.columns if 'question' in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Filtering interesting traces </h2>\n",
    "\n",
    "We are mostly interested in traces that conclude or include at least one instance of a \"slow\" channel. This could be a question asked to the call center, a discussion initiated on the website (identified by the presence of WerkMap messages) or a cimplaint filed. We can achieve that by first moving to a trace representation and then filtering accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the list of activities per session will be ordered by time\n",
    "log.sort_values(['sessionid', 'startTime'], ascending=[True, True], inplace=True)\n",
    "\n",
    "# Define aggregations when looking at each session\n",
    "aggregations = {'event': lambda x: list(x), \n",
    "                'startTime': 'first', \n",
    "                'completeTime': 'last', \n",
    "                'gender': 'first'\n",
    "               }\n",
    "sessions = log.groupby('sessionid', as_index=False).agg(aggregations)\n",
    "sessions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Throughtput </h2>\n",
    "\n",
    "It is interesting to check the distribution of time spent during each session and perhaps relate it to other session atributes, like demographics etc. We define a session's duration as the time spent between the start of its first event and the complete of its last event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions['duration'] = sessions['completeTime'] - sessions['startTime']\n",
    "\n",
    "def compute_average(durations):\n",
    "    \"\"\"\n",
    "    Compute the average from a (potentially long) iterable of timedeltas.\n",
    "    \"\"\"\n",
    "    size = len(durations)\n",
    "    zero = datetime.timedelta(0)\n",
    "    try:\n",
    "        return sum(durations, zero) / len(durations)\n",
    "    except OverflowError:\n",
    "        print(\"Duration list too big, processing in chunks\")\n",
    "        return sum(durations[0:50000], zero) / size + sum(durations[50000: ], zero) / size\n",
    "                \n",
    "average_duration = compute_average(sessions['duration'])\n",
    "average_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions['sessionid'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_traces(df, *events, include=True):\n",
    "    \"\"\"\n",
    "    Only keep traces that include or exclude at least one of the events passed\n",
    "    \"\"\"\n",
    "    def any_in(a, b):\n",
    "        if include:\n",
    "            return any(i in a for i in b)\n",
    "        return all(i not in a for i in b)\n",
    "    \n",
    "    return df[df['event'].apply(any_in, args=(events,))]\n",
    "        \n",
    "def conditional_average(*events, include=True):\n",
    "    subset = filter_traces(sessions, *events, include=include)\n",
    "    return compute_average(subset['duration'])\n",
    "\n",
    "def to_log(session_df):\n",
    "    \"\"\"\n",
    "    Translate the session view back to the original format\n",
    "    \"\"\"\n",
    "    session_set = session_df['sessionid'].unique()\n",
    "    return log[log['sessionid'].isin(session_set)]\n",
    "    \n",
    "sad_events = ['Question', 'Werkmap message', 'Complaint']\n",
    "bad_path_avg = conditional_average(*sad_events, include=True)\n",
    "print('Traces that include slow communication take {} on average'.format(bad_path_avg))\n",
    "\n",
    "happy_path_avg = conditional_average(*sad_events, include=False)\n",
    "print('Traces that do NOT include slow communication take {} on average'.format(happy_path_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets filter our initial dataset. These are the different ways to do it:\n",
    "\n",
    "# 1. Just provide a single event to be included or excluded\n",
    "test = filter_traces(sessions, 'Question', include=False) # Only traces without a question will be included\n",
    "\n",
    "# 2. Provide multiple events to be included or excluded\n",
    "test = filter_traces(sessions, 'Question', 'File Complaint', include=True) # Only traces with a question or a complaint\n",
    "\n",
    "# 3. Provide a list of events to be included or excluded\n",
    "sad_events = ['Question']\n",
    "happy_paths = filter_traces(sessions, *sad_events, include=False) # Only happy paths will be included\n",
    "sad_paths = filter_traces(sessions, *sad_events, include=True) # Only unhappy paths will be included\n",
    "\n",
    "# Get a subset back to the original customerID based format\n",
    "happy_log = to_log(happy_paths)\n",
    "happy_log.to_csv('data/happy_log.csv',index=False)\n",
    "sad_log = to_log(sad_paths)\n",
    "sad_log.to_csv('data/sad_log.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create subsets of happy and sad path (Santiago)\n",
    "We select 30 % of the cases in the sad path and 10% of the cases on happy path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub_set(x,por):\n",
    "    \"\"\"\n",
    "    Subsample of the number of cases to model.\n",
    "    \"\"\"\n",
    "    random.seed(3)\n",
    "    name_cases = x['case'].unique()\n",
    "    n = int(len(name_cases)*por)\n",
    "    indices = np.random.permutation(name_cases.shape[0])\n",
    "    training_idx, test_idx = indices[:n], indices[n:]\n",
    "    training_idx, test_idx = name_cases[training_idx], name_cases[test_idx]\n",
    "    \n",
    "    training = x[x['case'].isin(training_idx)]\n",
    "    test = x[x['case'].isin(test_idx)]\n",
    "    print('The initail number of cases is {} '.format(len(name_cases)))\n",
    "    print('The number of cases selected is  {} '.format(training_idx.shape[0]))\n",
    "\n",
    "    return training\n",
    "    \n",
    "\n",
    "create_sub_set(happy_log.copy(),0.05).to_csv('data/happy_log_subsample.csv',index=False)\n",
    "create_sub_set(sad_log.copy(),0.05).to_csv('data/sad_log_subsample.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Question as desagregated activity (Santiago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sad = pd.read_excel('C:/Users/s157084/Google Drive/Projects Process Mining/Data/paths_taxonomy/sad_with_taxonomy.xlsx')\n",
    "#df_sad = pd.read_csv('C:/Users/s157084/Google Drive/Projects Process Mining/Data/paths_taxonomy/sad_with_taxonomy.csv')1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_question(df):\n",
    "    \n",
    "    df['domain_level_1'][df['event'] == 'Question'] = df['domain_level_1'][df['event'] == 'Question'] + '/Question'\n",
    "    df['domain_level_2'][df['event'] == 'Question'] = df['domain_level_2'][df['event'] == 'Question'] + '/Question'\n",
    "    df['domain_level_2'][df['event'] == 'Question'] = df['domain_level_2'][df['event'] == 'Question'] + '/Question'\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "df_sad_imp = impute_question(df_sad.copy())\n",
    "df_sad_imp.to_csv('C:/Users/s157084/Google Drive/Projects Process Mining/Data/paths_taxonomy/sad_with_taxonomy_Question.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sad_imp['domain_level_1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Are certain demographic groups more probable to use slow communication channels? </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(question_sessions[question_sessions['gender'] == 'V']) / len(question_sessions) # Hint: Not really :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file from the sad_path with the activities that trigger questions (Santiago)\n",
    "\n",
    "This subset is elaborate with the activities that where found in the markov model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger = ['Visit page mijn_werkmap',\n",
    " 'Visit page home',\n",
    " 'Visit page taken',\n",
    " 'Visit page vacatures_bij_mijn_cv',\n",
    " 'Visit page mijn_berichten',\n",
    " 'Visit page werkmap',\n",
    " 'Visit page mijn_documenten',\n",
    " 'Visit page mijn_sollicitaties',\n",
    " 'Visit page mijn_cv',\n",
    " 'Visit page mijn_tips',\n",
    " 'Visit page inschrijven',\n",
    " 'Visit page foutopgetreden.html']\n",
    "\n",
    "def to_log(log,session_df):\n",
    "    \"\"\"\n",
    "    Translate the session view back to the original format\n",
    "    \"\"\"\n",
    "    session_set = session_df['sessionid'].unique()\n",
    "    return log[log['sessionid'].isin(session_set)]\n",
    "\n",
    "\n",
    "sad_paths_Q = pd.read_csv('C:/Users/s157084/Google Drive/Projects Process Mining/Data/paths/sad_log_1Q_inclQ.csv', encoding = \"ISO-8859-1\")\n",
    "aggregations = {'event': lambda x: list(x), \n",
    "                'startTime': 'first', \n",
    "                'completeTime': 'last', \n",
    "                'gender': 'first'\n",
    "               }\n",
    "\n",
    "sessions = sad_paths_Q.groupby('sessionid', as_index=False).agg(aggregations)\n",
    "path_trigger = filter_traces(sessions, *trigger, include=True)\n",
    "path_trigger  = to_log(sad_paths_Q, path_trigger) \n",
    "path_trigger.to_csv('data/sad_path_trigger.csv',index=False)\n",
    "path_trigger.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
